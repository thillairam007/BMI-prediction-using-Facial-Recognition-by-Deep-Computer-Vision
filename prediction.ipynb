{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "geEirVAGRJLK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "import logging\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cassandra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8BPUOfO0T1Fk"
      },
      "outputs": [],
      "source": [
        "import face_recognition"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "retrive the data from cassandra DB (may it was hibernated , if it is happened run the next cell... )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8UXuxVTNRJLc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.0.0.6816\n"
          ]
        }
      ],
      "source": [
        "from cassandra.cluster import Cluster\n",
        "from cassandra.auth import PlainTextAuthProvider\n",
        "\n",
        "cloud_config= {\n",
        "  'secure_connect_bundle': 'secure-connect-sample.zip'\n",
        "}\n",
        "auth_provider = PlainTextAuthProvider('WZnAUhYtZZCrGJbMRAsWaNUO', 'AoGWhtEbRLx89e+QwIa0qUd,y_AAdyl_8KkJv1Uguc+etLhArJgZg+3_tSkHSdD--s2PMAPoZna7ALlHC2pH_7aI+P9ddSe375qFbKjvFqhabfHW5tY3H5g8wz_tL76A')\n",
        "cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)\n",
        "session = cluster.connect()\n",
        "\n",
        "row = session.execute(\"select release_version from system.local\").one()\n",
        "if row:\n",
        "  print(row[0])\n",
        "else:\n",
        "  print(\"An error occurred.\")\n",
        "\n",
        "query = \"SELECT * FROM csv.bmidataset\"\n",
        "df = pd.DataFrame(list(session.execute(query)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "reading the csv file using pandas "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "EB7GZ8pNRJLo"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv(\"bmi data set.csv\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "creating the log file "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "logging.basicConfig(filename='bmi.log', level=logging.INFO, format='%(asctime)s:%(levelname)s:%(message)s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Aps4wTSZRJLq"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path as p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Q-CAVjkJRJLr"
      },
      "outputs": [],
      "source": [
        "#to identify the image name and iterate starts from 0\n",
        "\n",
        "def get_index_of_digit(string):\n",
        "    import re\n",
        "    match = re.search(\"\\d\", p(string).stem)\n",
        "    return match.start(0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "defining the data folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "J_f8Dtj7RJLt"
      },
      "outputs": [],
      "source": [
        "data_folder = \"sample_faces\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfHKwoeORJMq",
        "outputId": "3a3f235f-0346-4751-f6b2-c914d06fa8ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total 257 photos \n"
          ]
        }
      ],
      "source": [
        "from glob import glob\n",
        "all_files = glob(data_folder+\"/*\")\n",
        "\n",
        "all_jpgs = sorted([img for img in all_files if \".jpg\" in img or \".jpeg\" in img or \"JPG\" or \"png\" or \"PNG\" in img])\n",
        "logging.info(\"Total %d photos\", len(all_jpgs))\n",
        "print(\"Total {} photos \".format(len(all_jpgs)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "K4NFOaQrRJMu"
      },
      "outputs": [],
      "source": [
        "id_dir = [(p(images).stem[:(get_index_of_digit(p(images).stem))],images) for  images in all_jpgs ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1sTj4ai8RJMw"
      },
      "outputs": [],
      "source": [
        "image_df = pd.DataFrame(id_dir,columns=['UID','path'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SLok4X9fRJMz"
      },
      "outputs": [],
      "source": [
        "data_df = image_df.merge(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "MgPJxaHARJM0",
        "outputId": "e95a5f3f-46e5-408f-89ba-0f7c52f20f77"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UID</th>\n",
              "      <th>path</th>\n",
              "      <th>id</th>\n",
              "      <th>name</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>BMI</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>akshay</td>\n",
              "      <td>sample_faces\\akshay1.jpeg</td>\n",
              "      <td>1</td>\n",
              "      <td>akshay kumar</td>\n",
              "      <td>1.78</td>\n",
              "      <td>80</td>\n",
              "      <td>25.2493372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>akshay</td>\n",
              "      <td>sample_faces\\akshay10.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>akshay kumar</td>\n",
              "      <td>1.78</td>\n",
              "      <td>80</td>\n",
              "      <td>25.2493372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>akshay</td>\n",
              "      <td>sample_faces\\akshay11.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>akshay kumar</td>\n",
              "      <td>1.78</td>\n",
              "      <td>80</td>\n",
              "      <td>25.2493372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>akshay</td>\n",
              "      <td>sample_faces\\akshay12.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>akshay kumar</td>\n",
              "      <td>1.78</td>\n",
              "      <td>80</td>\n",
              "      <td>25.2493372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>akshay</td>\n",
              "      <td>sample_faces\\akshay13.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>akshay kumar</td>\n",
              "      <td>1.78</td>\n",
              "      <td>80</td>\n",
              "      <td>25.2493372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>224</th>\n",
              "      <td>vikky</td>\n",
              "      <td>sample_faces\\vikky5.jpg</td>\n",
              "      <td>8</td>\n",
              "      <td>vicky kaushal</td>\n",
              "      <td>1.83</td>\n",
              "      <td>80</td>\n",
              "      <td>23.88844098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>225</th>\n",
              "      <td>vikky</td>\n",
              "      <td>sample_faces\\vikky6.jpg</td>\n",
              "      <td>8</td>\n",
              "      <td>vicky kaushal</td>\n",
              "      <td>1.83</td>\n",
              "      <td>80</td>\n",
              "      <td>23.88844098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226</th>\n",
              "      <td>vikky</td>\n",
              "      <td>sample_faces\\vikky7.jpg</td>\n",
              "      <td>8</td>\n",
              "      <td>vicky kaushal</td>\n",
              "      <td>1.83</td>\n",
              "      <td>80</td>\n",
              "      <td>23.88844098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>227</th>\n",
              "      <td>vikky</td>\n",
              "      <td>sample_faces\\vikky8.jpg</td>\n",
              "      <td>8</td>\n",
              "      <td>vicky kaushal</td>\n",
              "      <td>1.83</td>\n",
              "      <td>80</td>\n",
              "      <td>23.88844098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>228</th>\n",
              "      <td>vikky</td>\n",
              "      <td>sample_faces\\vikky9.jpg</td>\n",
              "      <td>8</td>\n",
              "      <td>vicky kaushal</td>\n",
              "      <td>1.83</td>\n",
              "      <td>80</td>\n",
              "      <td>23.88844098</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>229 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        UID                       path  id           name height  weight  \\\n",
              "0    akshay  sample_faces\\akshay1.jpeg   1   akshay kumar   1.78      80   \n",
              "1    akshay  sample_faces\\akshay10.jpg   1   akshay kumar   1.78      80   \n",
              "2    akshay  sample_faces\\akshay11.jpg   1   akshay kumar   1.78      80   \n",
              "3    akshay  sample_faces\\akshay12.jpg   1   akshay kumar   1.78      80   \n",
              "4    akshay  sample_faces\\akshay13.jpg   1   akshay kumar   1.78      80   \n",
              "..      ...                        ...  ..            ...    ...     ...   \n",
              "224   vikky    sample_faces\\vikky5.jpg   8  vicky kaushal   1.83      80   \n",
              "225   vikky    sample_faces\\vikky6.jpg   8  vicky kaushal   1.83      80   \n",
              "226   vikky    sample_faces\\vikky7.jpg   8  vicky kaushal   1.83      80   \n",
              "227   vikky    sample_faces\\vikky8.jpg   8  vicky kaushal   1.83      80   \n",
              "228   vikky    sample_faces\\vikky9.jpg   8  vicky kaushal   1.83      80   \n",
              "\n",
              "             BMI  \n",
              "0     25.2493372  \n",
              "1     25.2493372  \n",
              "2     25.2493372  \n",
              "3     25.2493372  \n",
              "4     25.2493372  \n",
              "..           ...  \n",
              "224  23.88844098  \n",
              "225  23.88844098  \n",
              "226  23.88844098  \n",
              "227  23.88844098  \n",
              "228  23.88844098  \n",
              "\n",
              "[229 rows x 7 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "defining the function for face data extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WnvejAv5RJM1"
      },
      "outputs": [],
      "source": [
        "def my_face_encoding(image_path):\n",
        "    print(image_path)\n",
        "    logging.info(\"Getting face encoding for image %s\", image_path)\n",
        "    picture_of_me = face_recognition.load_image_file(image_path)\n",
        "    my_face_encoding = face_recognition.face_encodings(picture_of_me)\n",
        "    if not my_face_encoding:\n",
        "        print(\"no face found !!!\")\n",
        "        logging.warning(\"No face found in image %s\", image_path)\n",
        "        return np.zeros(128).tolist()\n",
        "    return my_face_encoding[0].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-d2GTl9DRJM3"
      },
      "outputs": [],
      "source": [
        "tot_faces = []"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "iteration and appending"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugSwX83rRJM8",
        "outputId": "9dc8d7bd-707f-4208-f8d1-52771b15a621"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sample_faces\\akshay1.jpeg\n",
            "sample_faces\\akshay10.jpg\n",
            "sample_faces\\akshay11.jpg\n",
            "sample_faces\\akshay12.jpg\n",
            "sample_faces\\akshay13.jpg\n",
            "sample_faces\\akshay14.jpg\n",
            "sample_faces\\akshay15.jpg\n",
            "sample_faces\\akshay16.jpg\n",
            "sample_faces\\akshay17.jpg\n",
            "sample_faces\\akshay18.jpg\n",
            "sample_faces\\akshay19.jpg\n",
            "sample_faces\\akshay2.jpeg\n",
            "no face found !!!\n",
            "sample_faces\\akshay20.jpg\n",
            "sample_faces\\akshay3.jpg\n",
            "sample_faces\\akshay4.jpg\n",
            "sample_faces\\akshay5.jpg\n",
            "sample_faces\\akshay6.jpg\n",
            "sample_faces\\akshay7.jpg\n",
            "sample_faces\\akshay8.jpg\n",
            "sample_faces\\akshay9.jpg\n",
            "sample_faces\\amir1.jpg\n",
            "sample_faces\\amir10.jpg\n",
            "sample_faces\\amir11.jpg\n",
            "sample_faces\\amir12.jpg\n",
            "sample_faces\\amir13.jpg\n",
            "sample_faces\\amir14.jpeg\n",
            "sample_faces\\amir15.jpg\n",
            "sample_faces\\amir2.jpg\n",
            "sample_faces\\amir3.jpg\n",
            "sample_faces\\amir4.jpeg\n",
            "no face found !!!\n",
            "sample_faces\\amir5.jpg\n",
            "sample_faces\\amir6.PNG\n",
            "sample_faces\\amir7.jpeg\n",
            "sample_faces\\amir8.jpg\n",
            "sample_faces\\amir9.jpg\n",
            "sample_faces\\anupam2.jpg\n",
            "sample_faces\\anupam3.jpg\n",
            "sample_faces\\anupam4.jpg\n",
            "sample_faces\\anupam5.jpg\n",
            "sample_faces\\anupam6.jpg\n",
            "sample_faces\\anupam7.jpeg\n",
            "sample_faces\\anupam8.jpg\n",
            "sample_faces\\anurag1.jpg\n",
            "sample_faces\\anurag2.jpg\n",
            "sample_faces\\anurag3.jpg\n",
            "sample_faces\\anurag4.jpg\n",
            "sample_faces\\anurag5.jpg\n",
            "sample_faces\\arshad1.jpg\n",
            "sample_faces\\arshad10.jpg\n",
            "sample_faces\\arshad11.jpg\n",
            "sample_faces\\arshad12.jpeg\n",
            "sample_faces\\arshad13.jpg\n",
            "sample_faces\\arshad14.jpg\n",
            "sample_faces\\arshad15.jpg\n",
            "sample_faces\\arshad16.jpg\n",
            "sample_faces\\arshad2.jpg\n",
            "sample_faces\\arshad3.jpeg\n",
            "sample_faces\\arshad4.jpg\n",
            "sample_faces\\arshad5.jpg\n",
            "sample_faces\\arshad6.jpg\n",
            "sample_faces\\arshad7.jpeg\n",
            "sample_faces\\arshad8.jpeg\n",
            "sample_faces\\arshad9.jpg\n",
            "sample_faces\\ayushman1.jpg\n",
            "sample_faces\\ayushman2.jpeg\n",
            "sample_faces\\ayushman3.JPG\n",
            "sample_faces\\ayushman4.jpg\n",
            "sample_faces\\ayushman5.jpg\n",
            "sample_faces\\ja1.jpg\n",
            "sample_faces\\ja10.jpg\n",
            "sample_faces\\ja11.jpg\n",
            "sample_faces\\ja12.jpg\n",
            "sample_faces\\ja13.jpeg\n",
            "sample_faces\\ja14.jpg\n",
            "sample_faces\\ja15.jpg\n",
            "sample_faces\\ja2.jpg\n",
            "sample_faces\\ja3.jpg\n",
            "sample_faces\\ja4.jpg\n",
            "sample_faces\\ja5.jpg\n",
            "sample_faces\\ja6.jpg\n",
            "sample_faces\\ja7.jpg\n",
            "sample_faces\\ja8.jpg\n",
            "sample_faces\\ja9.jpg\n",
            "sample_faces\\kalki1.jpg\n",
            "sample_faces\\kalki10.jpg\n",
            "sample_faces\\kalki11.jpg\n",
            "sample_faces\\kalki12.jpg\n",
            "sample_faces\\kalki13.jpeg\n",
            "sample_faces\\kalki14.jpg\n",
            "sample_faces\\kalki15.jpg\n",
            "sample_faces\\kalki2.jpg\n",
            "sample_faces\\kalki3.jpeg\n",
            "sample_faces\\kalki4.jpg\n",
            "sample_faces\\kalki5.jpg\n",
            "sample_faces\\kalki6.jpg\n",
            "sample_faces\\kalki7.jpg\n",
            "sample_faces\\kalki8.jpg\n",
            "sample_faces\\kalki9.jpg\n",
            "sample_faces\\kirron1.jpg\n",
            "sample_faces\\kirron10.jpg\n",
            "sample_faces\\kirron2.jpg\n",
            "sample_faces\\kirron3.jpg\n",
            "sample_faces\\kirron4.jpg\n",
            "sample_faces\\kirron5.jpg\n",
            "sample_faces\\kirron6.jpg\n",
            "sample_faces\\kirron7.jpg\n",
            "sample_faces\\kirron8.jpg\n",
            "sample_faces\\kirron9.jpeg\n",
            "sample_faces\\manoj1.jpg\n",
            "sample_faces\\manoj2.jpeg\n",
            "sample_faces\\manoj3.jpg\n",
            "sample_faces\\manoj4.jpg\n",
            "sample_faces\\manoj5.JPG\n",
            "sample_faces\\nawaz1.jpg\n",
            "sample_faces\\nawaz2.jpg\n",
            "sample_faces\\nawaz3.jpg\n",
            "sample_faces\\nawaz4.jpg\n",
            "sample_faces\\nawaz5.png\n",
            "sample_faces\\pankaj1.jpg\n",
            "sample_faces\\pankaj2.jpg\n",
            "sample_faces\\pankaj3.jpg\n",
            "sample_faces\\pankaj5.jpg\n",
            "sample_faces\\pankaj6.jpg\n",
            "sample_faces\\radhika1.jpg\n",
            "sample_faces\\radhika10.jpg\n",
            "sample_faces\\radhika11.jpg\n",
            "sample_faces\\radhika12.jpg\n",
            "sample_faces\\radhika13.jpg\n",
            "sample_faces\\radhika14.jpg\n",
            "sample_faces\\radhika15.jpg\n",
            "sample_faces\\radhika2.jpeg\n",
            "sample_faces\\radhika3.jpg\n",
            "sample_faces\\radhika4.jpg\n",
            "sample_faces\\radhika5.jpg\n",
            "sample_faces\\radhika6.jpg\n",
            "sample_faces\\radhika7.jpg\n",
            "sample_faces\\radhika8.jpg\n",
            "sample_faces\\radhika9.jpg\n",
            "sample_faces\\rajkumar1.jpg\n",
            "sample_faces\\rajkumar2.jpg\n",
            "sample_faces\\rajkumar3.jpg\n",
            "sample_faces\\rajkumar4.jpg\n",
            "sample_faces\\rajkumar5.jpg\n",
            "sample_faces\\ratna1.jpeg\n",
            "sample_faces\\ratna2.jpg\n",
            "sample_faces\\ratna3.jpg\n",
            "sample_faces\\ratna4.JPG\n",
            "sample_faces\\ratna5.jpg\n",
            "sample_faces\\ratna6.jpg\n",
            "sample_faces\\richa2.jpg\n",
            "sample_faces\\richa3.JPG\n",
            "sample_faces\\richa4.jpg\n",
            "sample_faces\\richa5.jpg\n",
            "sample_faces\\salman1.jpg\n",
            "sample_faces\\salman10.jpg\n",
            "sample_faces\\salman11.jpg\n",
            "sample_faces\\salman12.jpg\n",
            "sample_faces\\salman13.jpg\n",
            "sample_faces\\salman14.jpg\n",
            "sample_faces\\salman15.jpg\n",
            "sample_faces\\salman16.jpg\n",
            "sample_faces\\salman17.jpg\n",
            "sample_faces\\salman18.jpg\n",
            "sample_faces\\salman19.jpg\n",
            "sample_faces\\salman2.jpg\n",
            "sample_faces\\salman20.jpg\n",
            "sample_faces\\salman3.jpg\n",
            "sample_faces\\salman4.jpg\n",
            "sample_faces\\salman5.jpeg\n",
            "sample_faces\\salman6.jpg\n",
            "sample_faces\\salman7.jpg\n",
            "sample_faces\\salman8.jpg\n",
            "sample_faces\\salman9.jpg\n",
            "sample_faces\\srk1.jpg\n",
            "sample_faces\\srk10.jpeg\n",
            "sample_faces\\srk11.jpg\n",
            "sample_faces\\srk12.jpg\n",
            "sample_faces\\srk13.jpg\n",
            "sample_faces\\srk14.jpg\n",
            "sample_faces\\srk15.jpg\n",
            "sample_faces\\srk16.jpg\n",
            "sample_faces\\srk17.jpg\n",
            "sample_faces\\srk18.jpg\n",
            "sample_faces\\srk19.jpg\n",
            "sample_faces\\srk2.jpg\n",
            "sample_faces\\srk3.jpeg\n",
            "no face found !!!\n",
            "sample_faces\\srk4.jpg\n",
            "sample_faces\\srk5.jpg\n",
            "sample_faces\\srk6.jpg\n",
            "sample_faces\\srk7.jpg\n",
            "sample_faces\\srk8.jpg\n",
            "sample_faces\\srk9.jpg\n",
            "sample_faces\\supriya1.jpg\n",
            "sample_faces\\supriya2.jpg\n",
            "sample_faces\\supriya3.jpg\n",
            "sample_faces\\supriya4.jpg\n",
            "sample_faces\\supriya5.jpg\n",
            "sample_faces\\supriya6.jpg\n",
            "sample_faces\\tiger1.jpg\n",
            "sample_faces\\tiger2.jpg\n",
            "sample_faces\\tiger4.jpg\n",
            "sample_faces\\tiger5.jpg\n",
            "sample_faces\\tiger6.jpg\n",
            "sample_faces\\varun1.jpg\n",
            "sample_faces\\varun2.jpg\n",
            "sample_faces\\varun3.jpg\n",
            "sample_faces\\varun4.jpeg\n",
            "sample_faces\\varun5.png\n",
            "sample_faces\\varun6.jpeg\n",
            "sample_faces\\vikky1.jpg\n",
            "sample_faces\\vikky10.jpg\n",
            "sample_faces\\vikky11.jpg\n",
            "sample_faces\\vikky12.jpg\n",
            "sample_faces\\vikky13.jpg\n",
            "sample_faces\\vikky14.jpg\n",
            "sample_faces\\vikky15.jpg\n",
            "sample_faces\\vikky16.jpg\n",
            "sample_faces\\vikky17.jpg\n",
            "sample_faces\\vikky18.jpg\n",
            "sample_faces\\vikky19.jpg\n",
            "sample_faces\\vikky2.jpg\n",
            "sample_faces\\vikky20.jpg\n",
            "sample_faces\\vikky3.jpg\n",
            "sample_faces\\vikky4.jpg\n",
            "sample_faces\\vikky5.jpg\n",
            "sample_faces\\vikky6.jpg\n",
            "sample_faces\\vikky7.jpg\n",
            "sample_faces\\vikky8.jpg\n",
            "sample_faces\\vikky9.jpg\n"
          ]
        }
      ],
      "source": [
        "for images in data_df.path:\n",
        "    face_enc = my_face_encoding(images)\n",
        "    tot_faces.append(face_enc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "J_RJ8zMURJM9"
      },
      "outputs": [],
      "source": [
        "X = np.array(tot_faces)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ErowbAzJRJM-"
      },
      "outputs": [],
      "source": [
        "y_height = data_df.height.values\n",
        "y_weight = data_df.weight.values"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "splitting the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "AJivJdslRJND"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "d2v1xmWbRJNF"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_height_train, y_height_test, y_weight_train, y_weight_test = train_test_split(X, y_height,y_weight, random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9olli-IRJNH"
      },
      "source": [
        "Converting the data Type into float"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1sPyMGDQRJNP"
      },
      "outputs": [],
      "source": [
        "y_height_train=y_height_train.astype(float)\n",
        "y_height_test=y_height_test.astype(float)\n",
        "y_weight_train=y_weight_train.astype(float)\n",
        "y_weight_test=y_weight_test.astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZaZO_J4RJNU"
      },
      "source": [
        "getting the shape of the Training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T97noavQRJNV",
        "outputId": "7eec7dae-5f0f-4456-e90b-93ca0475d941"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(171, 128)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw61Oqo0RJNY"
      },
      "source": [
        "Train the model using CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScfSGjZnRJNZ"
      },
      "source": [
        "Developing height model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dB8VNPvRJNZ",
        "outputId": "0bc726d4-2943-4293-b120-d5f65ba7cb92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/13\n",
            "171/171 [==============================] - 5s 11ms/step - loss: 0.0235 - mae: 0.0862\n",
            "Epoch 2/13\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0020 - mae: 0.0321\n",
            "Epoch 3/13\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0015 - mae: 0.0273\n",
            "Epoch 4/13\n",
            "171/171 [==============================] - 2s 12ms/step - loss: 9.6012e-04 - mae: 0.0223\n",
            "Epoch 5/13\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 6.8759e-04 - mae: 0.0209\n",
            "Epoch 6/13\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 6.0829e-04 - mae: 0.0197\n",
            "Epoch 7/13\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 5.7538e-04 - mae: 0.0194\n",
            "Epoch 8/13\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 6.1515e-04 - mae: 0.0191\n",
            "Epoch 9/13\n",
            "171/171 [==============================] - 2s 12ms/step - loss: 4.7773e-04 - mae: 0.0171\n",
            "Epoch 10/13\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 5.8292e-04 - mae: 0.0195\n",
            "Epoch 11/13\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 6.9583e-04 - mae: 0.0198\n",
            "Epoch 12/13\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 4.2484e-04 - mae: 0.0161\n",
            "Epoch 13/13\n",
            "171/171 [==============================] - 2s 12ms/step - loss: 3.6530e-04 - mae: 0.0151\n",
            "2/2 [==============================] - 1s 16ms/step - loss: 5.6131e-04 - mae: 0.0182\n",
            "loss of height: 0.0005613135290332139\n",
            "mae of height: 0.018189096823334694\n"
          ]
        }
      ],
      "source": [
        "# Define the model architecture\n",
        "model_height = tf.keras.Sequential([\n",
        "    tf.keras.layers.Reshape((X_train.shape[1], 1), input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "    tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "    tf.keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(units=128, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=1, activation='relu')\n",
        "    ])\n",
        "model_height.compile(loss='mse', optimizer='adam',metrics=['mae'])\n",
        "\n",
        "# Fit the model to the training data\n",
        "model_height.fit(X_train, np.log(y_height_train), epochs=13, batch_size=1)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss_height_cnn,test_height_accuracy = model_height.evaluate(X_test, np.log(y_height_test))\n",
        "\n",
        "#logging \n",
        "logging.info(\"model height loss : %f, mae : %f\", test_loss_height_cnn,test_height_accuracy)\n",
        "\n",
        "#printing the metrics\n",
        "print('loss of height:', test_loss_height_cnn)\n",
        "print('mae of height:', test_height_accuracy)\n",
        "\n",
        "#saving the model\n",
        "model_height.save('height_model.h5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0v16IVyrRJNc"
      },
      "source": [
        "Developing weight model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D05eeD6KgWMw",
        "outputId": "7548fd97-7a89-44de-b6bb-61e9186fbc4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/13\n",
            "171/171 [==============================] - 5s 10ms/step - loss: 0.9494 - mae: 0.4875\n",
            "Epoch 2/13\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.1085 - mae: 0.2428\n",
            "Epoch 3/13\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0793 - mae: 0.2039\n",
            "Epoch 4/13\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0602 - mae: 0.1807\n",
            "Epoch 5/13\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0503 - mae: 0.1623\n",
            "Epoch 6/13\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0335 - mae: 0.1321\n",
            "Epoch 7/13\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0338 - mae: 0.1398\n",
            "Epoch 8/13\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0292 - mae: 0.1289\n",
            "Epoch 9/13\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0268 - mae: 0.1277\n",
            "Epoch 10/13\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0192 - mae: 0.1065\n",
            "Epoch 11/13\n",
            "171/171 [==============================] - 2s 13ms/step - loss: 0.0219 - mae: 0.1145\n",
            "Epoch 12/13\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0124 - mae: 0.0873\n",
            "Epoch 13/13\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0138 - mae: 0.0920\n",
            "2/2 [==============================] - 1s 11ms/step - loss: 0.0046 - mae: 0.0539\n",
            "loss of weight: 0.004626257810741663\n",
            "mae of weight: 0.05387092009186745\n"
          ]
        }
      ],
      "source": [
        "model_weight = tf.keras.Sequential([\n",
        "    tf.keras.layers.Reshape((X_train.shape[1], 1), input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "    tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "    tf.keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=1, activation='relu')\n",
        "])\n",
        "\n",
        "model_weight.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
        "\n",
        "model_weight.fit(X_train, np.log(y_weight_train), epochs=13, batch_size=1)\n",
        "\n",
        "test_loss_weight_cnn, test_weight_accuracy = model_weight.evaluate(X_test, np.log(y_weight_test))\n",
        "\n",
        "logging.info(\"model weight loss : %f, mae : %f\", test_loss_weight_cnn,test_weight_accuracy)\n",
        "print('loss of weight:', test_loss_weight_cnn)\n",
        "print('mae of weight:', test_weight_accuracy)\n",
        "\n",
        "#saving the model\n",
        "model_weight.save('weight_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "OGsejAFARJNp"
      },
      "outputs": [],
      "source": [
        "#load the models\n",
        "\n",
        "height_model = tf.keras.models.load_model('height_model.h5')\n",
        "weight_model = tf.keras.models.load_model('weight_model.h5')\n",
        "\n",
        "\n",
        "def predict_height_weight_BMI(input_img,height_model,weight_model):\n",
        "    logging.info(\"Predicting height, weight, and BMI for image %s\", input_img)\n",
        "    start_time = time.time()\n",
        "    test_array = np.expand_dims(np.array(my_face_encoding(input_img)),axis=0)\n",
        "    height = np.ndarray.item(np.exp(height_model.predict(test_array)))\n",
        "    weight = np.ndarray.item(np.exp(weight_model.predict(test_array)))\n",
        "    bmi = weight / (height)**2\n",
        "    end_time = time.time()\n",
        "    runtime = end_time - start_time\n",
        "    logging.info(\"Predicted height: %f, weight: %f, BMI: %f, runtime : %s\", height, weight, bmi, runtime)\n",
        "    return {'height':height,\"weight\":weight,\"bmi\":bmi,'runtime':runtime}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRZVeViRRJNq"
      },
      "source": [
        "for code level deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "KXh5Zg_gRJNr"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "hcz9nSQJRJNs",
        "outputId": "15138acf-a003-4671-f52d-d73aabd3e192"
      },
      "outputs": [
        {
          "data": {
            "image/jpeg": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_img = 'sample_faces/akshay1.jpeg'\n",
        "Image(input_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45rIX_9xRJNu",
        "outputId": "1d3bbb48-9b96-49df-95e8-54a5eaccb470"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sample_faces/akshay1.jpeg\n",
            "1/1 [==============================] - 0s 487ms/step\n",
            "1/1 [==============================] - 0s 393ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'height': 1.7140802145004272,\n",
              " 'weight': 74.26966094970703,\n",
              " 'bmi': 25.27837530517264,\n",
              " 'runtime': 2.7309978008270264}"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_height_weight_BMI(input_img,height_model,weight_model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Rbx3VdwyRJNv"
      },
      "source": [
        "creating GUI using tkinter ( use \"q\" to capture image )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "8fuEYpkHRJNw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ROCKRAM\\AppData\\Local\\Temp\\ipykernel_3180\\2701433283.py:29: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  image = image.resize((300, 300), Image.ANTIALIAS)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C:/Users/ROCKRAM/Desktop/project1/sample_faces/akshay5.jpg\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000020701800820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000020701B61430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 1s 1s/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ROCKRAM\\AppData\\Local\\Temp\\ipykernel_3180\\2701433283.py:51: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  image = image.resize((300, 300), Image.ANTIALIAS)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "captured_image.jpg\n",
            "1/1 [==============================] - 0s 357ms/step\n",
            "1/1 [==============================] - 0s 400ms/step\n",
            "captured_image.jpg\n",
            "no face found !!!\n",
            "1/1 [==============================] - 0s 400ms/step\n",
            "1/1 [==============================] - 0s 252ms/step\n",
            "captured_image.jpg\n",
            "1/1 [==============================] - 0s 178ms/step\n",
            "1/1 [==============================] - 0s 210ms/step\n"
          ]
        }
      ],
      "source": [
        "import tkinter as tk\n",
        "from tkinter import filedialog\n",
        "from PIL import ImageTk, Image\n",
        "import cv2\n",
        "\n",
        "\n",
        "# Load the height, weight, and BMI models\n",
        "height_model = tf.keras.models.load_model('height_model.h5')\n",
        "weight_model = tf.keras.models.load_model('weight_model.h5')\n",
        "\n",
        "\n",
        "# Define the function to predict height, weight, and BMI\n",
        "def predict_height_weight_BMI(input_img,height_model,weight_model):\n",
        "    logging.info(\"Predicting height, weight, and BMI for image %s\", input_img)\n",
        "    test_array = np.expand_dims(np.array(my_face_encoding(input_img)),axis=0)\n",
        "    height = np.ndarray.item(np.exp(height_model.predict(test_array)))\n",
        "    weight = np.ndarray.item(np.exp(weight_model.predict(test_array)))\n",
        "    bmi = weight / (height)**2\n",
        "    logging.info(\"Predicted height: %f, weight: %f, BMI: %f\", height, weight, bmi)\n",
        "    return {'height':height,\"weight\":weight,\"bmi\":bmi}\n",
        "\n",
        "# Define a function to get the file path of the selected image\n",
        "def browse_file():\n",
        "    global photo # Add this line to access the global variable\n",
        "    file_path = filedialog.askopenfilename()\n",
        "    if file_path:\n",
        "        # Open the selected image and display it in the UI\n",
        "        image = Image.open(file_path)\n",
        "        image = image.resize((300, 300), Image.ANTIALIAS)\n",
        "        photo = ImageTk.PhotoImage(image)\n",
        "        canvas.itemconfigure(image_id, image=photo)\n",
        "        canvas.image = photo\n",
        "\n",
        "        # Call the predict_height_weight_BMI() function and display the results in the UI\n",
        "        result = predict_height_weight_BMI(file_path, height_model, weight_model)\n",
        "        result_label.config(text=f\"Height: {result['height']:.2f}\\nWeight: {result['weight']:.2f}\\nBMI: {result['bmi']:.2f}\")\n",
        "\n",
        "def tocapture_image():\n",
        "    global photo # Add this line to access the global variable\n",
        "    cap = cv2.VideoCapture(0)\n",
        "    cap.set(3,640) # set Width\n",
        "    cap.set(4,480) # set Height\n",
        "    while(True):\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            cv2.imshow('frame', frame)\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                # Save the captured image and display it in the UI\n",
        "                cv2.imwrite('captured_image.jpg', frame)\n",
        "                image = Image.open('captured_image.jpg')\n",
        "                image = image.resize((300, 300), Image.ANTIALIAS)\n",
        "                photo = ImageTk.PhotoImage(image)\n",
        "                canvas.itemconfigure(image_id, image=photo)\n",
        "                canvas.image = photo\n",
        "\n",
        "                # Call the predict_height_weight_BMI() function and display the results in the UI\n",
        "                result = predict_height_weight_BMI('captured_image.jpg', height_model, weight_model)\n",
        "                result_label.config(text=f\"Height: {result['height']:.2f}\\nWeight: {result['weight']:.2f}\\nBMI: {result['bmi']:.2f}\")\n",
        "                break\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "# Define a function to clear the selected image and results\n",
        "def toclear_display():\n",
        "    global photo # Add this line to access the global variable\n",
        "    canvas.delete(image_id)\n",
        "    result_label.config(text=\"\")\n",
        "    photo = None # Set the global variable to None to allow the PhotoImage object to be garbage collected\n",
        "\n",
        "# Create the main window\n",
        "root = tk.Tk()\n",
        "root.title(\"Predict Height, Weight, and BMI\")\n",
        "\n",
        "# Create the canvas to display the image\n",
        "canvas = tk.Canvas(root, width=300, height=300)\n",
        "canvas.pack()\n",
        "\n",
        "# Create a label to display the results\n",
        "result_label = tk.Label(root, font=(\"Arial\", 14),bg='yellow')\n",
        "result_label.pack(pady=10)\n",
        "\n",
        "\n",
        "# Create a frame for the buttons\n",
        "button_frame = tk.Frame(root)\n",
        "button_frame.pack(pady=10)\n",
        "\n",
        "# Create a button to browse for an image\n",
        "browse_button = tk.Button(button_frame, text=\"Browse\", command=browse_file ,bg='skyblue')\n",
        "browse_button.pack(side=tk.LEFT, padx=10)\n",
        "\n",
        "# Create a button to capture image from the camera\n",
        "capture_button = tk.Button(button_frame, text=\"Capture\", command=tocapture_image,bg='pink')\n",
        "capture_button.pack(side=tk.LEFT, padx=10)\n",
        "\n",
        "# Create a button to clear the displayed image and results\n",
        "clear_button = tk.Button(button_frame, text=\"Clear\", command=toclear_display)\n",
        "clear_button.pack(side=tk.RIGHT, padx=10)\n",
        "\n",
        "# Initialize the image_id variable\n",
        "image_id = canvas.create_image((0, 0), anchor='nw')\n",
        "\n",
        "\n",
        "\n",
        "# Start the main loop\n",
        "root.mainloop()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "e3420ed2c65300ffc1d71199033e9176e0c26ff8ced7474b2f05f6e440a26153"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
